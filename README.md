# ğŸ­ Speech Emotion Recognition Using Dual-Layer LSTM

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8+-blue.svg" alt="Python">
  <img src="https://img.shields.io/badge/TensorFlow-2.x-orange.svg" alt="TensorFlow">
  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License">
  <img src="https://img.shields.io/badge/Status-In%20Progress-yellow.svg" alt="Status">
</p>

## ğŸ“„ About the Paper

This project implements the research paper **"Improvement and Implementation of a Speech Emotion Recognition Model Based on Dual-Layer LSTM"** by Xiaoran Yang, Shuhan Yu, and Wenxi Xu.

<p align="center">
  <img src="path/to/your/paper_screenshot.png" alt="Paper Header" width="700">
</p>

### ğŸ“š Paper Overview

The paper presents an enhanced speech emotion recognition (SER) system that builds upon existing models by introducing an additional LSTM layer to improve accuracy and computational efficiency. 

**Key Innovations:**
- ğŸ§  **Dual-Layer LSTM Architecture**: Captures long-term dependencies in audio sequences
- ğŸ“ˆ **2% Accuracy Improvement**: Outperforms single-layer LSTM models
- âš¡ **Reduced Latency**: Enhanced real-time performance
- ğŸ¯ **Complex Pattern Recognition**: Better extraction of emotional features from noisy audio

The dual-layer architecture addresses limitations of single-layer LSTM structures in extracting emotional features from audio data, especially when dealing with noisy or complex emotional shifts in speech.

---

## ğŸ¯ Project Objective

We are implementing this paper **from scratch** as a comprehensive machine learning project. This is a complete ground-up implementation where we:

âœ… Design and code the dual-layer LSTM architecture from scratch  
âœ… Implement custom feature extraction pipelines for audio data  
âœ… Train the model on emotion-labeled speech datasets  
âœ… Evaluate performance metrics against baseline models  
âœ… Reproduce and validate the research findings  

---

## ğŸ“Š Dataset: RAVDESS

We are using the **RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)** dataset, which was also validated in the original paper.

### Dataset Characteristics

| Feature | Description |
|---------|-------------|
| **Actors** | 24 professional actors (12 male, 12 female) |
| **Emotions** | 7 categories: neutral, calm, happy, sad, angry, fearful, disgust, surprised |
| **Modality** | Audio recordings of emotional speech |
| **Quality** | Professionally recorded with controlled acoustic conditions |
| **Balance** | Equal representation across emotions and genders |

### Why RAVDESS?

- âœ¨ Industry-standard benchmark for SER research
- ğŸ¯ Clean, labeled data ideal for supervised learning
- ğŸ“Š Sufficient size for training deep learning models
- ğŸ”¬ Enables direct comparison with paper's reported results

**Dataset Link:** [RAVDESS on Kaggle](https://www.kaggle.com/uwrfkaggle/ravdess-emotional-speech-audio)

---

## ğŸ—ï¸ Architecture

### Dual-Layer LSTM Model
```
Input (Audio Features)
        â†“
   LSTM Layer 1 (128 units)
        â†“
     Dropout (0.3)
        â†“
   LSTM Layer 2 (64 units)
        â†“
     Dropout (0.3)
        â†“
   Dense Layer (64 units, ReLU)
        â†“
   Output Layer (7 units, Softmax)
```

### Feature Extraction Pipeline

- **MFCC** (Mel-Frequency Cepstral Coefficients)
- **Chroma Features**
- **Mel Spectrogram**
- **Zero Crossing Rate**
- **Spectral Centroid**

---

## ğŸ› ï¸ Technologies Used

- **Python 3.8+**
- **TensorFlow / Keras** - Deep Learning Framework
- **Librosa** - Audio Processing
- **NumPy** - Numerical Computing
- **Pandas** - Data Manipulation
- **Matplotlib / Seaborn** - Visualization
- **Scikit-learn** - ML Utilities

---

## ğŸ“ Project Structure
```
speech-emotion-recognition/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Raw RAVDESS dataset
â”‚   â”œâ”€â”€ processed/              # Preprocessed features
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_feature_extraction.ipynb
â”‚   â”œâ”€â”€ 03_model_training.ipynb
â”‚   â””â”€â”€ 04_evaluation.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_preprocessing.py   # Audio preprocessing functions
â”‚   â”œâ”€â”€ feature_extraction.py   # Feature extraction utilities
â”‚   â”œâ”€â”€ model.py                # Dual-Layer LSTM model
â”‚   â”œâ”€â”€ train.py                # Training script
â”‚   â””â”€â”€ evaluate.py             # Evaluation script
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ saved_models/           # Trained model checkpoints
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ plots/                  # Visualization outputs
â”‚   â””â”€â”€ metrics/                # Performance metrics
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## ğŸš€ Getting Started

### Prerequisites
```bash
Python 3.8 or higher
pip or conda package manager
```

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/speech-emotion-recognition.git
cd speech-emotion-recognition
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Download RAVDESS dataset**
```bash
# Download from Kaggle or official source
# Place in data/raw/ directory
```

---

## ğŸ’» Usage

### 1. Preprocess Data
```bash
python src/data_preprocessing.py --data_path data/raw/ --output_path data/processed/
```

### 2. Extract Features
```bash
python src/feature_extraction.py --input_path data/processed/ --output_path data/features/
```

### 3. Train Model
```bash
python src/train.py --epochs 100 --batch_size 32 --learning_rate 0.001
```

### 4. Evaluate Model
```bash
python src/evaluate.py --model_path models/saved_models/best_model.h5
```

### 5. Run Inference
```python
from src.model import DualLayerLSTM
from src.feature_extraction import extract_features

# Load model
model = DualLayerLSTM.load('models/saved_models/best_model.h5')

# Predict emotion
audio_file = 'path/to/audio.wav'
features = extract_features(audio_file)
emotion = model.predict(features)
print(f"Predicted Emotion: {emotion}")
```

---

## ğŸ“ˆ Results

### Expected Performance (Based on Paper)

| Metric | Single-Layer LSTM | Dual-Layer LSTM (Our Implementation) |
|--------|------------------|--------------------------------------|
| Accuracy | ~XX% | ~XX% (+2%) |
| Precision | ~XX% | ~XX% |
| Recall | ~XX% | ~XX% |
| F1-Score | ~XX% | ~XX% |

> **Note:** Results will be updated as we progress with the implementation.

---

## ğŸ—ºï¸ Roadmap

- [x] Project setup and planning
- [ ] Data collection and preprocessing
- [ ] Feature extraction implementation
- [ ] Dual-Layer LSTM model architecture
- [ ] Model training and validation
- [ ] Performance evaluation
- [ ] Hyperparameter tuning
- [ ] Real-time emotion recognition interface
- [ ] Documentation and final report

---

## ğŸ¤ Contributing

We welcome contributions! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¥ Team

- **Your Name** - [GitHub](https://github.com/yourusername) | [LinkedIn](https://linkedin.com/in/yourprofile)
- **Teammate 2** - [GitHub](https://github.com/teammate2)

---

## ğŸ™ Acknowledgments

- Original Paper Authors: Xiaoran Yang, Shuhan Yu, and Wenxi Xu
- RAVDESS Dataset Creators
- Open-source community for tools and libraries

---

## ğŸ“§ Contact

For questions or feedback, please reach out:
- Email: your.email@example.com
- Project Link: [https://github.com/yourusername/speech-emotion-recognition](https://github.com/yourusername/speech-emotion-recognition)

---

<p align="center">Made with â¤ï¸ and ğŸµ</p>
