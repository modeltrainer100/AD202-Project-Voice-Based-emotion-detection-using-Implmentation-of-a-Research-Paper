# ğŸ­ Voice-Based Emotion Detection Using Dual-Layer LSTM

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)
![Keras](https://img.shields.io/badge/Keras-D00000?style=for-the-badge&logo=keras&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)

**An advanced Speech Emotion Recognition system achieving 83.26% accuracy**

[Research Paper](#-research-foundation) â€¢ [Architecture](#-architecture-deep-dive) â€¢ [Results](#-performance-metrics) â€¢ [Demo](#-live-demo) â€¢ [Getting Started](#-quick-start)

</div>

---

## ğŸŒŸ Project Highlights

<table>
<tr>
<td width="50%">

### ğŸ¯ **Achievement Unlocked**
- **83.26%** Test Accuracy
- **<100ms** Inference Time
- **11,680** Training Samples
- **8** Emotion Classes

</td>
<td width="50%">

### ğŸš€ **Key Features**
- Dual-layer LSTM architecture
- Real-time emotion prediction
- Interactive web interface
- Comprehensive visualization

</td>
</tr>
</table>

---

## ğŸ“Š Performance Journey

```
Initial Model (REVDAS only)
â”œâ”€ Dataset Size: 1,440 samples
â”œâ”€ Accuracy Range: 57% - 73%
â””â”€ Best Accuracy: 73.88%

Final Model (Expanded Dataset)
â”œâ”€ Dataset Size: 11,680 samples
â”œâ”€ Accuracy Range: 76% - 83%
â””â”€ Best Accuracy: 83.26% âœ¨
```

### ğŸ“ˆ Improvement Breakdown

| Metric | Initial Model | Final Model | Improvement |
|--------|--------------|-------------|-------------|
| **Test Accuracy** | 73.88% | 83.26% | **+9.38%** |
| **Test Loss** | 0.71 | 0.52 | **-26.8%** |
| **Dataset Size** | 1,440 | 11,680 | **+710%** |

---

## ğŸ“ Research Foundation

### Paper Implementation
This project implements and extends the methodology from:

> **"Improvement and Implementation of a Speech Emotion Recognition Model Based on Dual-Layer LSTM"**  
> *by Xiaoran Yang, Shuhan Yu, and Wenxi Xu*

### ğŸ”¬ Our Contributions

1. **ğŸ“¦ Dataset Expansion** - Integrated multiple datasets for robust training
2. **ğŸ›ï¸ Hyperparameter Optimization** - 500+ configurations tested
3. **ğŸ–¥ï¸ Production Deployment** - Streamlit-based interactive application
4. **ğŸ“Š Comprehensive Analysis** - In-depth performance evaluation

---

## ğŸ—‚ï¸ Datasets Used

<div align="center">

| Dataset | Samples | Speakers | Description |
|---------|---------|----------|-------------|
| **REVDAS** | 1,440 | - | Initial training baseline |
| **RAVDESS** | âœ“ | 24 actors | Professional emotional speech |
| **CREMA-D** | âœ“ | - | Diverse speaker demographics |
| **SAVEE** | âœ“ | - | British English speakers |
| **TESS** | âœ“ | - | Toronto emotional speech |
| **MELD** | âœ“ | - | Multi-party conversations |

**Total Training Samples:** 11,680 files

</div>

### ğŸ¯ Emotion Classes

```python
emotions = [
    'ğŸ˜  Angry',
    'ğŸ˜Œ Calm', 
    'ğŸ¤¢ Disgust',
    'ğŸ˜¨ Fearful',
    'ğŸ˜Š Happy',
    'ğŸ˜ Neutral',
    'ğŸ˜¢ Sad',
    'ğŸ˜² Surprised'
]
```

---

## ğŸ—ï¸ Architecture Deep Dive

### Model Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Input Audio (3 seconds)              â”‚
â”‚               22,050 Hz, Mono                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Feature Extraction Pipeline            â”‚
â”‚  â€¢ 40 MFCCs + Deltas                           â”‚
â”‚  â€¢ Chroma Features                              â”‚
â”‚  â€¢ Mel Spectrogram                              â”‚
â”‚  â€¢ Spectral Contrast                            â”‚
â”‚  â€¢ Tonnetz                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       LSTM Layer 1 (256 units)                  â”‚
â”‚       return_sequences=True                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Batch Normalization + Dropout (0.4)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       LSTM Layer 2 (128 units)                  â”‚
â”‚       return_sequences=False                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Batch Normalization + Dropout (0.5)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Dense Layer (256 units, ReLU)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Output Layer (8 units, Softmax)             â”‚
â”‚        Emotion Probabilities                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ¼ Feature Extraction Details

| Feature Type | Dimensions | Purpose |
|-------------|-----------|---------|
| **MFCCs** | 40 coefficients | Capture timbre and spectral envelope |
| **Delta MFCCs** | 40 coefficients | Track temporal changes |
| **Chroma** | 12 bins | Represent harmonic content |
| **Mel Spectrogram** | Variable | Frequency representation |
| **Spectral Contrast** | 7 bands | Texture information |
| **Tonnetz** | 6 features | Harmonic relationships |

---

## ğŸ”§ Optimal Configuration

After 500+ hyperparameter searches, the winning configuration:

```python
{
    "architecture": {
        "lstm_1_units": 256,
        "lstm_2_units": 128,
        "dense_units": 256,
        "dropout_lstm": 0.4,
        "dropout_dense": 0.5
    },
    "training": {
        "batch_size": 128,
        "learning_rate": 0.001,
        "optimizer": "Adam",
        "loss": "sparse_categorical_crossentropy"
    },
    "preprocessing": {
        "sample_rate": 22050,
        "duration": 3.0,
        "n_mfcc": 40
    }
}
```

---

## ğŸ“Š Performance Metrics

### ğŸ¯ Confusion Matrix Insights

**High Performance Emotions:**
- âœ… **Angry** - Distinctive high arousal patterns
- âœ… **Fearful** - Clear acoustic signatures
- âœ… **Disgust** - Strong spectral characteristics
- âœ… **Calm** - Low arousal, stable features

**Challenging Emotions:**
- âš ï¸ **Happy vs Sad** - Similar energy profiles
- âš ï¸ **Neutral** - Overlaps with low-arousal states

### âš¡ Inference Performance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   End-to-End Processing Pipeline     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Audio Loading:         ~20ms        â”‚
â”‚  Feature Extraction:    ~50ms        â”‚
â”‚  Model Inference:       ~15ms        â”‚
â”‚  Visualization:         ~10ms        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Total Time:           <100ms âœ¨      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ–¥ï¸ Live Demo

### Streamlit Application Features

<table>
<tr>
<td width="33%" align="center">

#### ğŸ“¤ Upload
Drag & drop or browse audio files

</td>
<td width="33%" align="center">

#### ğŸ”® Predict
Real-time emotion detection

</td>
<td width="33%" align="center">

#### ğŸ“Š Visualize
Interactive charts & plots

</td>
</tr>
</table>

### Sample Output

```
ğŸµ Audio Analysis Complete!

Detected Emotion: ğŸ˜Š Happy (87.3% confidence)

Emotion Probabilities:
  Happy:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 87.3%
  Surprised: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 8.2%
  Neutral:   â–ˆâ–ˆâ–ˆ 2.1%
  Calm:      â–ˆâ–ˆ 1.4%
  Others:    â–ˆ 1.0%
```

---

## ğŸš€ Quick Start

### Prerequisites

```bash
Python 3.8+
pip or conda
```

### Installation

```bash
# Clone repository
git clone https://github.com/your-username/emotion-recognition-lstm.git
cd emotion-recognition-lstm

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### ğŸ’» Usage

#### Train Model
```bash
python src/train.py \
  --dataset expanded \
  --epochs 100 \
  --batch_size 128 \
  --lstm_units 256 128
```

#### Run Inference
```python
from src.model import EmotionRecognizer

# Initialize model
model = EmotionRecognizer.load('models/best_model.h5')

# Predict emotion
emotion, confidence = model.predict('audio_sample.wav')
print(f"Emotion: {emotion} ({confidence:.2%})")
```

#### Launch Web App
```bash
streamlit run app.py
```

---

## ğŸ“ Project Structure

```
emotion-recognition-lstm/
â”‚
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ raw/              # Original datasets
â”‚   â”œâ”€â”€ processed/        # Preprocessed audio
â”‚   â””â”€â”€ features/         # Extracted features
â”‚
â”œâ”€â”€ ğŸ“‚ models/
â”‚   â”œâ”€â”€ checkpoints/      # Training checkpoints
â”‚   â””â”€â”€ final/            # Best model (83.26%)
â”‚
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ preprocessing.py  # Audio preprocessing
â”‚   â”œâ”€â”€ features.py       # Feature extraction
â”‚   â”œâ”€â”€ model.py          # LSTM architecture
â”‚   â”œâ”€â”€ train.py          # Training pipeline
â”‚   â””â”€â”€ evaluate.py       # Performance metrics
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/
â”‚   â”œâ”€â”€ 01_EDA.ipynb
â”‚   â”œâ”€â”€ 02_Features.ipynb
â”‚   â”œâ”€â”€ 03_Training.ipynb
â”‚   â””â”€â”€ 04_Analysis.ipynb
â”‚
â”œâ”€â”€ ğŸ“‚ app/
â”‚   â””â”€â”€ streamlit_app.py  # Web interface
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt
â”œâ”€â”€ ğŸ“„ README.md
â””â”€â”€ ğŸ“„ report.pdf
```

---

## ğŸ”¬ Key Findings

### 1ï¸âƒ£ Dataset Size Matters Most

Expanding from 1,440 to 11,680 samples provided the **largest performance boost** (+9.38% accuracy), far exceeding architectural improvements alone.

### 2ï¸âƒ£ Dual-Layer LSTM Effectiveness

The hierarchical structure proves essential:
- **Layer 1:** Captures short-term acoustic details
- **Layer 2:** Learns abstract emotional patterns

### 3ï¸âƒ£ Regularization is Critical

Dropout rates of 0.4-0.5 prevented overfitting while maintaining strong generalization.

### 4ï¸âƒ£ Feature Diversity Helps

Combining MFCCs, Chroma, Mel-spectrograms, and Spectral Contrast created a robust representation resistant to speaker variability.

---

## ğŸ¯ Limitations & Future Work

### Current Limitations

- ğŸ¬ **Acted Speech:** Limited to professional recordings
- ğŸŒ **English Only:** No multilingual support
- ğŸ”‡ **Clean Audio:** Minimal noise robustness testing
- ğŸ“Š **Discrete Classes:** Continuous emotion space not modeled

### ğŸš€ Future Improvements

- [ ] Real-world spontaneous speech testing
- [ ] Multi-language emotion recognition
- [ ] Noise robustness enhancement
- [ ] Continuous emotion dimension modeling
- [ ] Attention mechanism integration
- [ ] Transfer learning from pre-trained models

---

## ğŸ‘¥ Team

**Indian Institute of Information Technology (IIIT) Raichur**

- **Aditya Upendra Gupta** (AD24B1003)
- **Anshika Agarwal** (AD24B1007)
- **Kartavya Gupta** (AD24B1028)

**Supervisor:** Dr. Dubacharla Gyaneshwar

---

## ğŸ“š References

<details>
<summary>Click to expand full reference list</summary>

1. Yang, X., Yu, S., & Xu, W. "Improvement and Implementation of a Speech Emotion Recognition Model Based on Dual-Layer LSTM"

2. Livingstone, S. R., & Russo, F. A. (2018). "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)", *PLOS ONE*

3. Hochreiter, S., & Schmidhuber, J. (1997). "Long Short-Term Memory", *Neural Computation*

4. El Ayadi, M., Kamel, M. S., & Karray, F. (2011). "Survey on Speech Emotion Recognition", *Pattern Recognition*

5. McFee, B., et al. (2015). "librosa: Audio and Music Signal Analysis in Python"

</details>

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- RAVDESS Dataset creators
- TensorFlow/Keras development team
- Streamlit for deployment framework
- Research paper authors for methodology

---

<div align="center">

### â­ If you find this project useful, please consider giving it a star!

**Made with â¤ï¸ by IIIT Raichur Students**

[Report Issues](https://github.com/your-username/emotion-recognition-lstm/issues) â€¢ [Request Features](https://github.com/your-username/emotion-recognition-lstm/issues)

</div>
